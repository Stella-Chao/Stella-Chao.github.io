---
title: GPT/GPT2阅读笔记
date: 2023-03-12 19:43:00 +0800
categories: [NLP, Paper notes]
tags:     # TAG names should always be lowercase
math: true
---
# 1. Language Models are Unsupervised Multitask Learners （GPT2）

1. 在一些NLP任务（例如问答，翻译，自动摘要）中，使用监督训练存在一些问题：
> 通用性差：依赖数据分布和任务规范
> 
> 
2. 在无监督情况下，文章在WebText数据集（数百万的webpages）进行了实验
    1. 语言模型的容量对于任务转移很重要
    2. 以文档+问题输入，CoQA数据集F1值达到55
3. GPT-2: 1.5B参数的Transformer: 探索建立通用语言处理系统可行性
4. 目标：建立一个大语言模型，实现

## 1.1 相关工作
1. 多任务学习，元学习
2. 目前流行的NLP任务有：
    1. 有监督：预训练 + 微调，序列标注等
    2. 弱监督无监督：常识推理、情感分析
> 本文主要连接以上两条线的工作，尝试零触发（无需架构或者参数修改）执行下游任务

## 1.2 方法
1. 因为自然语言以词序列形式（例如：$(s_1, s_2, s_3...s_n)$）表示，可使用条件概率作为语言模型建模：\\
$$
p(x)=\prod_{i=1}^n p(s_n|s_1...s_n-1)
$$
2. 计算条件概率$p(s_{n-k},...s_n \| s_1,..s_{n-k-1})$以预估上下文的词概率，例如Tranformer
3. 对于多任务通用的系统，对于不同的任务，模型的形式\$p(输出\|输入, 任务)\$。如多任务学习和元学习
4. 系统的训练输入可写成序列，如$(翻译为英文，中文，英文)，(回答问题，文档，问题，回答)$
5. 监督目标与非监督目标相同，但仅对序列的子集进行评估，问题转换为优化无监督目标实现收敛

## 1.3 训练集
### 1.3.1 WebText
1. Common Crawl数据集，数量大并且有质量问题
2. 偏重高质量采集：WebText
### 1.3.2 输入表示
 （LM）应该能够计算（并生成）任何字符串的概率，如何表示语料库中的tokens，字节级-性能损失大。单词级-任务一般性弱？
> 1. 采用BPE算法，压缩token列表（将序列中频次最高的一对数据单位(Unicode)，替换为一个新的token） 
> 2. 禁止BPE对字节序列进行跨字符类别的合并，为防止如$dog. dog! dog?$情况，浪费token list

### 1.3.3 模型
采用Tansformer为基础与OpenAI GPT模型作为架构
## 1.4 实验：阅读理解，摘要，问答，翻译
### 1.4.1 